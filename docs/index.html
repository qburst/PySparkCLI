<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="X-UA-Compatible" content="ie=edge"/>
    <title>PySpark CLI Documentation</title>
    <link rel="stylesheet" href="resources/css/main.css"/>
    <link href="https://fonts.googleapis.com/css?family=Chewy" rel="stylesheet"/>
</head>
<body>
<div class="wrap">
    <nav id="navbar">
        <header>
            <h1>PySpark CLI Documentation</h1>
        </header>
        <ul>
            <a class="nav-link" href="#Introduction" rel="internal">
                <li>Introduction</li>
            </a>
            <a class="nav-link" href="#What_you_should_already_know" rel="internal">
                <li>What you should already know</li>
            </a>
            <a class="nav-link" href="#Apache_Spark_and_PySpark" rel="internal">
                <li>Apache Spark and PySpark</li>
            </a>
            <a class="nav-link" href="#Pre_Requisites" rel="internal">
                <li>Pre-Requisites</li>
            </a>
            <a class="nav-link" href="#Available_Commands" rel="internal">
                <li>Available Commands</li>
            </a>
            <a class="nav-link" href="#Writing_your_first_PySpark_App" rel="internal">
                <li>Writing your first PySpark App</li>
            </a>
            <a class="nav-link" href="#References" rel="internal">
                <li>References</li>
            </a>
            <a class="nav-link" href="#Credits" rel="internal">
                <li>Credits</li>
            </a>
        </ul>
    </nav>
    <main id="main-doc">
        <section class="main-section" id="Introduction">
            <header>Introduction</header>
            <article>
                <p>This documentation contains the step-by-step procedure to create a PySpark project using a CLI.</p>
            </article>
        </section>
        <section class="main-section" id="What_you_should_already_know">
            <header>What you should already know</header>
            <article>
                <p>Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in
                    Java, Scala, Python and R, and an optimized engine that supports general execution graphs.
                    It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data
                    processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
                </p>
                <p>PySpark is the Python API for Spark.</p>
            </article>
        </section>
        <section class="main-section" id="Apache_Spark_and_PySpark">
            <header>Apache Spark and PySpark</header>
            <article>
                <p>Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly)
                    in-memory data processing engine that can do ETL, analytics, machine learning and graph processing
                    on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich
                    concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL.
                </p>
                <p>You could also describe Spark as a distributed, data processing engine for batch and streaming modes
                    featuring SQL queries, graph processing, and machine learning.
                    In contrast to Hadoop’s two-stage disk-based MapReduce computation engine, Spark’s multi-stage
                    (mostly) in-memory computing engine allows for running most computations in memory, and hence most
                    of the time provides better performance for certain applications, e.g. iterative algorithms or
                    interactive data mining (read Spark officially sets a new record in large-scale sorting).
                    Spark aims at speed, ease of use, extensibility and interactive analytics.
                    Spark is often called cluster computing engine or simply execution engine.
                    Spark is a distributed platform for executing complex multi-stage applications, like machine
                    learning algorithms, and interactive ad hoc queries. Spark provides an efficient abstraction for
                    in-memory cluster computing called Resilient Distributed Dataset.
                    Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive
                    analytics at scale.
                    Spark is mainly written in Scala, but provides developer API for languages like Java, Python, and
                    R.
                </p>
                <p> Why Spark? </p>
                <li>Easy to Get Started - Spark offers spark-shell that makes for a very easy head start to writing and
                    running Spark applications on the command line on your laptop. You could then use Spark Standalone
                    built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a
                    full dataset.
                </li>
                <li>Unified Engine for Diverse Workloads - Spark combines batch, interactive, and streaming workloads
                    under one rich concise API.
                    Spark supports near real-time streaming workloads via Spark Streaming application framework.
                    ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified
                    platform for a wide variety of workloads.
                    Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers
                    over network means better performance.
                    There is also support for interactive workloads using Spark shell.
                </li>
                <li>Leverages the Best in distributed batch data processing - When you think about distributed batch
                    data processing, Hadoop naturally comes to mind as a viable solution. Spark draws many ideas out of
                    Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the
                    performance and simplicity of the distributed computing engine.
                    For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a
                    surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born
                    at all.
                </li>
                <li>Interactive Exploration / Exploratory Analytics - It is also called ad hoc queries. Using the Spark
                    shell you can execute computations to process large amount of data (The Big Data). It’s all
                    interactive and very useful to explore the data before final production release.
                </li>
                <li>RDD - Distributed Parallel Scala Collections
                    As a Scala developer, you may find Spark’s RDD API very similar (if not identical) to Scala’s
                    Collections API.
                    It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense).
                </li>
                <li>Rich Standard Library -
                    Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of
                    other higher-level operators to ease your Spark queries and application development.
                    It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop
                    MapReduce.
                </li>
                <li>Unified development and deployment environment for all -
                    Regardless of the Spark tools you use - the Spark API for the many programming languages supported -
                    Scala, Java, Python, R, or the Spark shell, or the many Spark Application Frameworks leveraging the
                    concept of RDD, i.e. Spark SQL, Spark Streaming, Spark MLlib and Spark GraphX, you still use the
                    same development and deployment environment to for large data sets to yield a result, be it a
                    prediction (Spark MLlib), a structured data queries (Spark SQL) or just a large distributed batch
                    (Spark Core) or streaming (Spark Streaming) computation.
                </li>
                <li>Single Environment -
                    Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you
                    can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying
                    your applications leveraging the many ingestion data points offered by the Spark platform.
                </li>
                <li>Data Integration Toolkit with Rich Set of Supported Data Sources -
                    Spark can read from many types of data sources — relational, NoSQL, file systems, etc. — using many
                    types of data formats - Parquet, Avro, CSV, JSON.
                </li>
                <li>Tools unavailable then, at your fingertips now - Spark embraces many concepts in a single unified
                    development and runtime environment.
                    Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used
                    by Scala developers (as Pipeline API in Spark MLlib or calling pipe()).
                    DataFrames from R are available in Scala, Java, Python, R APIs.
                    Single node computations in machine learning algorithms are migrated to their distributed versions
                    in Spark MLlib.
                </li>
                <li>Low-level Optimizations -
                    Apache Spark uses a directed acyclic graph (DAG) of computation stages (aka execution DAG). It
                    postpones any processing until really required for actions. Spark’s lazy evaluation gives plenty of
                    opportunities to induce low-level optimizations (so users have to know less to do more).
                </li>
                <li>Excels at low-latency iterative workloads -
                    Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are
                    often used in Machine Learning and graph algorithms.
                </li>
                <li>ETL done easier -
                    Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages
                    supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a
                    problem.
                    Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages
                    and approaches like MapReduce in Java).
                </li>
                <li>Unified Concise High-Level API -
                    Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset
                    API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing
                    (Graph API).
                </li>
                <li>Different kinds of data processing using unified API -
                    Spark offers three kinds of data processing using batch, interactive, and stream processing with the
                    unified API and data structures.
                </li>
                <li>Little to no disk use for better performance</li>
                <li>Fault Tolerance included</li>
                <li>Small Codebase Invites Contributors</li>
            </article>
        </section>
        <section class="main-section" id="Pre_Requisites">
            <header>Pre-Requisites</header>
            <p> Follow these steps to set-up the environment required for working with PySpark projects. For
                installation in Windows, follow the link <a
                        href="https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3">How
                    to Install Apache Spark on Windows</a>.
                For installation on Ubuntu, follow these steps:
            </p>
            <p>1. Download and Install JDK 8 or above.
                Before you can start with spark and hadoop, you need to make sure you have java 8 installed, or to
                install it.
            </p>
            <article>
                You should check Java by running following command::
                <code> java -version
                </code>
                Select the code in the pad and hit Ctrl+R to watch it unfold in your browser!
            </article>
            <p>If JDK 8 is not installed you should follow the tutorial <a
                    href="https://www.roseindia.net/answers/viewqa/linux/32404-How-to-Install-Oracle-Java-JDK-8-in-Ubuntu-16-04-.html">How
                to Install Oracle Java JDK 8 in Ubuntu 16.04?</a>
            </p>
            <p>2. Download and install Apache Spark
                Now the next step is to download latest distribution of Spark. Visit the website <a
                        href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a> and
                there you will find the latest distribution of Spark framework.
            </p>
            <article>Create a directory spark with following command in your home.
                <code>mkdir spark</code>
            </article>
            <article>
                Move spark-2.4.4-bin-hadoop2.7.tgz in the spark directory:
                <p>
                    <code>mv ~/Downloads/spark-2.3.0-bin-hadoop2.7.tgz spark <br>cd spark/ <br>tar -xzvf <br>spark-2.4.4-bin-hadoop2.7.tgz
                    </code>
                </p>
                After extracting the file go to bin directory of spark and run ./pyspark.
                It will open following pyspark shell:
                <p>
                    <img src="./resources/images/spark_shell.png" alt="pyspark_shell" />
                </p>
            </article>
            <p>3. Configure Apache Spark
                Now you should configure it in path so that it can be executed from anywhere.
            </p>
            <article>
                Open bash_profile file:
                <code> vi ~/.bash_profile
                </code>
                Add following entry:
                <code>export SPARK_HOME=~/spark/spark-2.4.4-bin-hadoop2.7/ <br>export PATH="$SPARK_HOME/bin:$PATH"
                </code>
                Run the following command to update PATH variable in the current session:
                <code>source ~/.bash_profile
                </code>
                After next login you should be able to find pyspark command in path and it can be accessed from any
                directory.
            </article>
            <p>4. Check PySpark installation </p>
            <p>
                <article>
                    In your anaconda prompt,or any python supporting cmd, type pyspark, to enter pyspark shell. To be
                    prepared, best to check it in the python environment from which you run jupyter notebook.
                    You are supposed to see the following:
                </article>
                <img src=".git/resources/images/spark_shell.png" alt="pyspark_shell" />
                <article>Run the following commands, the output should be [1,4,9,16].
                    <code>$ pyspark <br>>>> nums = sc.parallelize([1,2,3,4]) <br>>>> nums.map(lambda x: x*x).collect()
                    </code>
                    To exit pyspark shell, type Ctrl-z and enter. Or the python command exit()
                </article>
            </p>
            <p>5. Install PySpark using pip </p>
            <p>
                <article>
                    In your anaconda prompt,or any python supporting cmd, run the following command:
                    <code>pip install pyspark</code>
                </article>
                <article>Run the following commands, this should open up teh pyspark shell.
                    <code>pyspark
                    </code>
                    To exit pyspark shell, type Ctrl-z and enter. Or the python command exit()
                </article>
            </p>
        </section>
        <section class="main-section" id="Available_Commands">
            <header>Available Commands</header>
            <h3>1. create </h3>
            <article>
                <code>pysparkcli create --master [MASTER_URL] --name [PROJECT_NAME] --cores [NUMBER]
                </code>
            </article>
            <li><b>master </b> is the URL of the cluster it connects to. You can also use -m instead of --master.
            </li>
            <li><b>name </b>: The name of your PySpark project. You can also use -n instead of --name.
            </li>
            <li><b>cores </b>: This controls the number of parallel tasks an executor can run. You can also use -c
                instead of --cores.
            </li>
            <h3>2. run </h3>
            <article><code>pysparkcli run -n [PROJECT_NAME]
            </code>
            </article>
            <li><b>n </b>: The name of your PySpark project.
            </li>
            <br>
        </section>
        <section class="main-section" id="Writing_your_first_PySpark_App">
            <header>Writing your first PySpark App
            </header>
            <p>Let’s learn by example.
                We’ll assume you have PySpark installed already. You can tell PySpark is installed and which version by
                running the following command in a shell prompt (indicated by the $ prefix):
            </p>
            <code>$pyspark</code>
            <p>If PySpark is installed, you should see the version of your installation. If it isn’t, you’ll get an
                error.
                This tutorial is written for Spark 2.4.4, which supports Python 2.7.15 and later.
            </p>
            <br>
            <h3>Creating a project</h3>
            <p>
                If this is your first time using PySpark, you’ll have to take care of some initial setup. Namely, you’ll
                need to auto-generate some code that establishes a PySpark project – a collection of settings for an
                instance of PySpark.
            </p>
            <article>
                Run the follwing code to install <b>pysparkcli</b>
                <code>pip install pysparkcli</code>
                From the command line, cd into a directory where you’d like to store your code, then run the following
                command:
                <code>$ pysparkcli create -m local[*] -name sample</code>
            </article>
            <p>This will create a <b>sample</b> directory in your current directory.</p>
            <p>Let's look at what <b>create</b> has done.</p>
            <code>sample
                ├── __init__.py
                ├── src
                │ ├── app.py
                │ ├── configs
                │ │ └── __init__.py
                │ ├── __init__.py
                │ └── settings
                │ ├── default.py
                │ ├── __init__.py
                │ ├── local.py
                │ └── production.py
                └── tests
                └── __init__.py</code>
            <p>These files are:
            <li>The outer <b>sample/</b> root directory is just a container for your project.
            </li>
            <li><b>init.py</b> files are required to make Python treat the directories as containing packages.
            </li>
            <li><b>src</b></li>
            <li><b>app.py</b></li>
            <li><b>configs</b></li>
            <li><b>settings</b></li>
            <li><b>default.py</b></li>
            <li><b>local.py</b> includes the settings to be used for local environment.</li>
            <li><b>local.py</b> includes the settings to be used for local environment.</li>
            <li><b>tests</b> includes the unit tests which can be written to test the code.</li>
            </p>
            <br>
            <h3>The development server</h3>
            Run the follwing code to run your project:
            <code>pysparkcli run sample</code>
            You'll see the following in your command line:
            <code>Started running project: sample/
                19/11/25 10:37:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using
                builtin-java classes where applicable
                Hello World!.</code>
        </section>
        <section class="main-section" id="References">
            <header>References</header>
            <article>
                <p>All the documentation in this page is taken from: </p>
                <li><a href="https://spark.apache.org/docs/latest/" target="_blank">Apache Spark</a></li>
                <li><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html" target="_blank">PySpark</a>
                </li>
                <li><a href="https://spark.apache.org/downloads.html" target="_blank">Download Apache Spark</a></li>
            </article>
        </section>
        <section class="main-section" id="Credits">
            <header>Credits</header>
            <article>
                <li>Special credits to <a href="https://github.com/AlexIoannides/pyspark-example-project"
                                          target="_blank">https://github.com/AlexIoannides/pyspark-example-project</a>
                    and
                    <a href="https://github.com/ekampf/PySpark-Boilerplate" target="_blank">https://github.com/ekampf/PySpark-Boilerplate</a>
                    from where we formulated the basic structure for this PySpark project.
                </li>
            </article>
        </section>
    </main>
</div>
<footer>
    <!--<a href="https://www.disegnosis.com.ar" target="_blank">
        <img src="https://www.diseñowebs.com.ar/freecodecamp/responsive-web-design-projects/tribute-page/img/disegnosis.png"
             alt="Diseño Web DISEGNOSIS - Webmaster Diseño de Páginas / Sitios Web. Servicios de Hosting, Streaming y Mailing."/>
    </a>-->
</footer>
</body>
</html>
